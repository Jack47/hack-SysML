## 背景

目前 clip 的实现有几种，比如 : openai clip, [huggingface clip](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py) 。要求是找到给当前 CLIP 模型的推理过程（训练可以先不考虑）做加速的方法，实践之后给出加速比

## 成果：

最终呈现结果是下面的表格：

| 加速方法        | 加速比 |
| --------------- | ------ |
| Flash attention | 59%    |
|                 | y%     |

## 要求：

1.需要用文档呈现过程：在某个地方建个文档，

2.相关代码的组织：在 github 上创建一个项目，把自己做加速的过程体现到这个项目里，也可以是 fork huggingface clip，然后在里面增加自己的改动

## 提示

1.CLIP 里计算大头是 transformer，所以可以找到 flash attention 来加速它

 ## 结果

尝试利用https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn上FlashselfAttention类来修改clip中用到的Attenton类，来进行对比，但是报错找不到flash_attn_2_cuda库，尝试看了一下源码和配置环境，目前还没有解决该问题。

发现[baaivision/EVA: EVA Series: Visual Representation Fantasies from BAAI (github.com)](https://github.com/baaivision/EVA)上所写的attention部分已经集成了flash attention的使用，可以在attention源码中修改xatten=True来制定使用Flash attention，试了一下，使用flash attention时前向传播时间为5.76s， 不使用时间为9.17。

