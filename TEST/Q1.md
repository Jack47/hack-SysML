# 第一题: 计算几类典型的 CLIP 任务的 flops

## 背景

CLIP 论文里提到的 CLIP 架构的模型，有 ResNet 和 ViT 两种方式，现在我们希望能计算出使用了 ViT 的 CLIP，它的几种规格下的 TFlops。

## 最终成果

1.把下面表格里最后一列填好

| 模型规格                                                     | 模型参数量 | TFLOPs         |
| ------------------------------------------------------------ | ---------- | -------------- |
| https://huggingface.co/openai/clip-vit-large-patch14-336     | 400M       | 0.116512260096 |
| https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K | 2B         | 0.10798628864  |
| https://huggingface.co/QuanSun/EVA-CLIP                      | 4B         | 0.958556798976 |

2.回答后面的问题部分的几个问题

## 要求：

需要写出中间自己的思考过程，即把过程文档化，这样能体现出自己的思考，而且我也方便离线地评论。

## 问题

1. CLIP 模型的参数量和 CLIP 模型对应的 TFLOPS 之间是什么关系？

   答：通常情况下，CLIP模型参数量越大，其TFLOPS也越大(这也是一开始我认为的，比如表格中第二行和第三行)。

   但是在实际跑模型计算时，参数量400M比参数量2B的模型跑出来TFLOPs更大。通过进一步回顾分析，发现400M模型的输入尺寸是336 * 336，而2B模型的输入为224 * 224， 而都是14 14的patchsize。所以对于基于Vision Transformer的CLIP模型，计算TFLOPs通常依赖于输入大小和参数大小。

2. TFlops 有哪些计算方法？你使用了哪几种

   有估算法和实际计算法。
   估算法可以通过理论计算模型在每一层的运算量，然后计算得到总的计算量。比如，对于一个attention层，假设输入矩阵维度为n * h，Q、K、V矩阵维度都为h * h，那么在生成q时，会进行n * h * （2h - 1）次运算（乘和加），生成k、v时同理。所以在生成q、k、v时会进行3 * n * h * （2h - 1）次运算。此方法需要知道模型每一层的参数设置。

   实际计算法是我采用的方法。通过在模型运行时来计算FLOPS。这里我用到了PyTorch的profile，可以跟踪模型的运行时间和内存使用情况，并扩展到FLOPs的计算。

3. 是否能找到这个模型在不同的硬件，比如 A100、H800 等上面的 sota 的 MFU？

   可能需要查看硬件制造商发布的白皮书、研究论文等详细说明，这些资源可能会提供具体模型在这些硬件上运行的基准测试结果

4. 上面对应的 sota 指标是如何达到的？

   可以利用高效的深度学习框架（如TensorFlow, PyTorch）和针对特定硬件优化的库（如CUDA, cuDNN）。

   可以调整模型架构，比如调整batch大小，减少层数或参数，使用更高效的操作

   可以优化超参数，通过调整学习率、优化器选择等超参数来优化训练过程。

5. 我们使用的 clip 会切换调整，如何实现一个 python 函数快速计算当前给定的 clip 规格，返回 TFLOPs ？

   在计算TFOPS时，我使用了open_clip库加载模型，通过查阅相关文档和源码发现，这个库支持多种clip规格模型加载，可以使用它来加载模型，并用profile来计算FLOPS。(见py文件)

6. 有没有遇得到一些困难或者不理解的概念？具体有哪些可以展开说说

   在计算CLIP-ViT-H-14的参数量时，得到的结果接近1B，表格中给出的却是2B，暂时无法找到原因。